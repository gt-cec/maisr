{
	"notes": "1 target, agent observes vector to nearest target",
	"window_size": [1100,1100],
	"tick_rate": 10,
	"time_limit": 245,
	"game_speed": 0.07,
	"agent_speed": 2.8,
	"num_aircraft": 1,
	"max_steps": 30000,

	"num_targets": 15,
	"time_to_id": 0,
	"prob_detect": 0,

	"seed": 42,

	"obs_type": "nearest",
	"action_type": "Discrete16",
	"num_observed_targets": 4,
	"num_observed_threats": 1,
	"threat_radius": 50,

	"highqual_regulartarget_reward": 0,
	"highqual_highvaltarget_reward": 0,
	"shaping_decay_rate":1,
	"shaping_coeff_prox": 0.21,
	"shaping_coeff_earlyfinish": 0.0000,
	"shaping_time_penalty": 0.0000,
	"inside_threat_penalty": 0.03,

	"verbose": false,
	"algo": "PPO",
	"policy_type": "MLPPolicy",
	"policy_network_size": 64,
	"value_network_size": 64,
	"activation_fn": "Tanh",
	"lr": 0.0005,
	"batch_size": 512,
	"gamma": 0.99,
	"frame_skip": 30,
	"ppo_update_steps": 2048,
	"entropy_regularization": 0.015,
	"vf_coef":0.5,
	"n_epochs":10,
	"lr_schedule":"none",
	"clip_range": 0.2,

	"num_timesteps": 6e6,
	"save_freq": 13500,
	"n_eval_episodes": 5,
	"eval_freq": 13500,

	"use_curriculum": true,
	"cl_lr_decrease": 1,
	"levels_per_lesson": {"0": 50, "1": 50, "2":  50},
	"gameboard_size_per_lesson": {"0": 1000, "1": 1000, "2": 1000},
	"agent_start_locations_per_lesson": {"0": 99, "1": 99, "2": 99},

	"min_target_ids_to_advance": 8,
	"starting_difficulty": 0,
	"max_difficulty": 2
}
